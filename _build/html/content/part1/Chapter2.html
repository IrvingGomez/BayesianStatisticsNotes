

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>2. Uniparametric models &#8212; Bayesian Statistics</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" href="../../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'content/part1/Chapter2';</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="References" href="../../References.html" />
    <link rel="prev" title="1. Bayesian philosophy" href="Chapter1.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../../intro.html">
  
  
  
  
    
    
      
    
    
    <img src="../../_static/RF_logo.png" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="../../_static/RF_logo.png" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../intro.html">
                    Bayesian statistics: Course notes
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">I Bayesian foundations</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="Chapter1.html">1. Bayesian philosophy</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">2. Uniparametric models</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../References.html">References</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/content/part1/Chapter2.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Uniparametric models</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#notation">2.1. Notation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#posterior-distribution">2.2. Posterior distribution</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#beta-binomial">2.3. Beta-Binomial</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example">2.3.1. Example</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#principle-of-indifference">2.4. Principle of indifference</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#proportion-of-girls-births">2.4.1. Proportion of girls’ births</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#probability-of-a-girl-birth-given-placenta-previa">2.4.2. Probability of a girl birth given placenta previa</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cells-production-of-protein">2.4.3. Cells production of protein</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#predictive-distributions">2.5. Predictive distributions</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#rule-of-succession">2.5.1. Rule of succession</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#determining-the-hyperparameters-in-the-beta-binomial-model">2.6. Determining the hyperparameters in the Beta-Binomial model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#normal-convergence-of-the-beta-binomial-model">2.7. Normal convergence of the Beta-Binomial model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#inference-of-a-fair-or-a-biased-coin">2.8. Inference of a fair or a biased coin</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#prior-distribution">2.8.1. Prior distribution</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#likelihood">2.8.2. Likelihood</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#prior-predictive-distribution">2.8.3. Prior predictive distribution</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#likelihood-of-the-sample">2.8.4. Likelihood of the sample</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#evidence">2.8.5. Evidence</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">2.8.6. Posterior distribution</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#posterior-predictive-distribution">2.8.7. Posterior predictive distribution</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#there-is-no-free-lunch">2.9. There is no free lunch</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="uniparametric-models">
<h1><span class="section-number">2. </span>Uniparametric models<a class="headerlink" href="#uniparametric-models" title="Permalink to this heading">#</a></h1>
<p>Consider a random variable <span class="math notranslate nohighlight">\(Y\)</span>, whose likelihood, when it takes the value <span class="math notranslate nohighlight">\(y\)</span> and the parameters <span class="math notranslate nohighlight">\(\theta\)</span> take the value <span class="math notranslate nohighlight">\(t\)</span>, is given by</p>
<div class="math notranslate nohighlight">
\[p_{Y|\theta}(y|t).\]</div>
<p>For example, if <span class="math notranslate nohighlight">\(Y\)</span> is variable with a normal distribution with mean <span class="math notranslate nohighlight">\(\theta\)</span> and unit variance, then</p>
<div class="math notranslate nohighlight">
\[p_{Y|\theta}(y|t)=\frac{1}{\sqrt{2\pi}}\exp\left\lbrace-\frac{(y-t)^2}{2}\right\rbrace 1_{y\in\mathbb{R}}.\]</div>
<p>In the Bayesian argot, it is usual to to make use of abused of notations, sometimes even dangerous, for example</p>
<div class="math notranslate nohighlight">
\[p_{Y|\theta}(y|t)\]</div>
<p>would be written as</p>
<div class="math notranslate nohighlight">
\[p(Y|\theta).\]</div>
<section id="notation">
<h2><span class="section-number">2.1. </span>Notation<a class="headerlink" href="#notation" title="Permalink to this heading">#</a></h2>
<p>Before moving forward, it is good to introduce at this point the notation that we are going to use throughout this book. However, sooner than later, we are going to follow the abuse of notation usual in the field of Bayesian statistics.</p>
<ul class="simple">
<li><p>Random variables: <span class="math notranslate nohighlight">\(Y, \theta\)</span>.</p></li>
<li><p>Sample space: <span class="math notranslate nohighlight">\(\mathcal{Y}\)</span>, <span class="math notranslate nohighlight">\(Y\in\mathcal{Y}\)</span>.</p></li>
<li><p>Parametric space: <span class="math notranslate nohighlight">\(\Theta\)</span>, <span class="math notranslate nohighlight">\(\theta\in\Theta\)</span>.</p></li>
<li><p>Random sample: <span class="math notranslate nohighlight">\(\mathbf{Y}=(Y_1,\ldots,Y_n)\)</span>.</p></li>
<li><p>Observed sample: <span class="math notranslate nohighlight">\(\mathbf{y}=(y_1,\ldots,y_n)\)</span>.</p></li>
<li><p>Prior distribution or <em>a priori</em>: <span class="math notranslate nohighlight">\(p(\theta)\)</span>.</p></li>
<li><p>Likelihood: <span class="math notranslate nohighlight">\(p(Y|\theta)\)</span>.</p></li>
<li><p>Likelihood of the sample: <span class="math notranslate nohighlight">\(p(\mathbf{Y}|\theta)\)</span>.</p></li>
<li><p>Evidence: <span class="math notranslate nohighlight">\(p(\mathbf{Y})\)</span>.</p></li>
<li><p>Posterior distribution or <em>a posteriori</em>: <span class="math notranslate nohighlight">\(p(\theta|\mathbf{Y})\)</span>.</p></li>
</ul>
</section>
<section id="posterior-distribution">
<h2><span class="section-number">2.2. </span>Posterior distribution<a class="headerlink" href="#posterior-distribution" title="Permalink to this heading">#</a></h2>
<p>The prior distribution of probability is the distribution based on the previous information (expertise of specialists, historical data, etc.), prior to obtain new measurements. Then, we get new data (we obtain evidence) and combine this new information with the prior distribution using Bayes’ rule to obtain the posterior distribution of probability:</p>
<div class="math notranslate nohighlight">
\[p_{\theta|\mathbf{Y}}(t|\mathbf{y})=\frac{p_{\mathbf{Y}|\theta}(\mathbf{y}|t)p_\theta(t)}{p_\mathbf{Y}(\mathbf{y})}=\frac{p_{\mathbf{Y}|\theta}(\mathbf{y}|t)p_\theta(t)}{\int_\Theta p_{\mathbf{Y}|\theta}(\mathbf{y}|\tilde t)p_\theta(\tilde t)d\tilde t}\]</div>
<p>With the usual abuse of notation of Bayesian statistics, the previous result would be written as:</p>
<div class="math notranslate nohighlight">
\[p(\theta|\mathbf{Y})=\frac{p(\mathbf{Y}|\theta)p(\theta)}{p(\mathbf{Y})}=\frac{p(\mathbf{Y}|\theta)p(\theta)}{\int_\Theta p(\mathbf{Y}|\tilde\theta)p(\tilde\theta)d\tilde\theta}\]</div>
<p>or, equivalentely:</p>
<div class="math notranslate nohighlight">
\[p(\theta|\mathbf{Y})\propto p(\mathbf{Y}|\theta)p(\theta).\]</div>
</section>
<section id="beta-binomial">
<h2><span class="section-number">2.3. </span>Beta-Binomial<a class="headerlink" href="#beta-binomial" title="Permalink to this heading">#</a></h2>
<p>Asume that <span class="math notranslate nohighlight">\(Y_i|\theta\overset{iid}{\sim}\text{Bernoulli}(\theta)\)</span> and the uncertainty about <span class="math notranslate nohighlight">\(\theta\in (0,1)\)</span> is quantified through <span class="math notranslate nohighlight">\(\theta\sim\textsf{Beta}(\alpha, \beta)\)</span>. We get the observations <span class="math notranslate nohighlight">\(\mathbf{y}=(y_1,\ldots,y_n)\)</span>, then</p>
<div class="math notranslate nohighlight">
\[p(\mathbf{Y}|\theta)=\theta^{\sum_{i=1}^n y_i}(1-\theta)^{n-\sum_{i=1}^n y_i}\prod_{i=1}^n1_{\{0,1\}}(y_i)\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[p(\theta)=B(\alpha,\beta)^{-1}\theta^{\alpha-1}(1-\theta)^{\beta-1}1_{(0,1)}(\theta).\]</div>
<p>Thus,</p>
<div class="math notranslate nohighlight">
\[p(\theta|\mathbf{Y})\propto \theta^{\alpha+\sum_{i=1}^n y_i-1}(1-\theta)^{\beta+n-\sum_{i=1}^n y_i-1}1_{(0,1)}(\theta),\]</div>
<p>that is,</p>
<div class="math notranslate nohighlight">
\[\theta|\mathbf{Y}\sim\textsf{Beta}\left(\alpha+\sum_{i=1}^n y_i, \beta+n-\sum_{i=1}^n y_i\right).\]</div>
<p>Remember that if <span class="math notranslate nohighlight">\(Y_1\ldots,Y_n|\theta\overset{iid}{\sim}\textsf{Bernoulli}(\theta)\)</span>, then</p>
<div class="math notranslate nohighlight">
\[Z=\sum_{i=1}^n Y_i|\theta\sim\textsf{Binomial}(n,\theta).\]</div>
<p>Therefore, if</p>
<div class="math notranslate nohighlight">
\[\theta\sim\textsf{Beta}(\alpha, \beta),\]</div>
<p>we conclude that</p>
<div class="math notranslate nohighlight">
\[\theta|Z\sim\textsf{Beta}(\alpha+Z,\beta+n-Z).\]</div>
<section id="example">
<h3><span class="section-number">2.3.1. </span>Example<a class="headerlink" href="#example" title="Permalink to this heading">#</a></h3>
<p>Assume that <span class="math notranslate nohighlight">\(Y|\theta\sim\textsf{Bernoulli}(\theta)\)</span> and that we don’t have information about <span class="math notranslate nohighlight">\(\theta\)</span> to prefer some value over another, so we model <span class="math notranslate nohighlight">\(\theta\sim\textsf{Beta}(1,1)\)</span> (i.e. <span class="math notranslate nohighlight">\(\theta\sim\textsf{Uniform}(0,1)\)</span>). Then, we get a simple random sample (i.e. we obtain an iid sample) <span class="math notranslate nohighlight">\(\mathbf{y}=(1,0,1,1,1,0,1,0,1)\)</span>, or equivalently <span class="math notranslate nohighlight">\(z=6\)</span> and <span class="math notranslate nohighlight">\(n=9\)</span>. Next figure shows the prior and posterior distributions.</p>
<img alt="PriorPosteriorBetaBinomial" class="align-center" src="../../_images/PriorPosteriorBetaBinomial.png" />
</section>
</section>
<section id="principle-of-indifference">
<h2><span class="section-number">2.4. </span>Principle of indifference<a class="headerlink" href="#principle-of-indifference" title="Permalink to this heading">#</a></h2>
<p>Analyzing the binomial model, Laplace assumed the uniform distribution as prior arguing what he called the <em>principle of indifference</em>, also called <em>principle of insufficient reason</em>, which stablishes that the uniform assumption is appropriate when we have no information about <span class="math notranslate nohighlight">\(\theta\)</span>, that is</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\theta &amp; \sim \textsf{Beta}(1,1), \\
Y|\theta &amp; \sim \textsf{Binomial}(n,\theta) \\
\Rightarrow \theta|Y &amp; \sim \textsf{Beta}(Y+1, n-Y+1).
\end{aligned}
\end{split}\]</div>
<section id="proportion-of-girls-births">
<h3><span class="section-number">2.4.1. </span>Proportion of girls’ births<a class="headerlink" href="#proportion-of-girls-births" title="Permalink to this heading">#</a></h3>
<p>One of the first applications of this model made by Laplace was the estimation of the proportion of girls’ births, <span class="math notranslate nohighlight">\(\theta\)</span>, in a population. Laplace knew that between 1745 and 1770, 241945 girls and 251527 boys were born in Paris, if <span class="math notranslate nohighlight">\(Y\)</span> denotes the number of girls’ births, then</p>
<div class="math notranslate nohighlight">
\[\theta|Y\sim\textsf{Beta}(241946, 251528).\]</div>
<p>With this result, we can show that it is more probable that a boy is born than a girl, as it is shown in the next cell, which calculates <span class="math notranslate nohighlight">\(\mathbb{P}(\theta&gt;0.5)\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">beta</span>

<span class="n">fem_births</span> <span class="o">=</span> <span class="mi">241945</span>
<span class="n">mal_births</span> <span class="o">=</span> <span class="mi">251527</span>

<span class="n">beta</span><span class="o">.</span><span class="n">sf</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">fem_births</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">mal_births</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1.1460584901546728e-42
</pre></div>
</div>
</div>
</div>
</section>
<section id="probability-of-a-girl-birth-given-placenta-previa">
<h3><span class="section-number">2.4.2. </span>Probability of a girl birth given placenta previa<a class="headerlink" href="#probability-of-a-girl-birth-given-placenta-previa" title="Permalink to this heading">#</a></h3>
<p>Placenta previa is an unusual condition of pregnancy in which the placenta is implanted low in the uterus. An early study concerning the sex of placenta previa births in Germany found of a total of 980 births, 437 were female.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>

<span class="kn">from</span> <span class="nn">scipy.special</span> <span class="kn">import</span> <span class="n">logit</span><span class="p">,</span> <span class="n">expit</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">births</span> <span class="o">=</span> <span class="mi">987</span>
<span class="n">fem_births</span> <span class="o">=</span> <span class="mi">437</span>
</pre></div>
</div>
</div>
</div>
<p>Posterior mean</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">beta</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">fem_births</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">births</span><span class="o">-</span><span class="n">fem_births</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.443
</pre></div>
</div>
</div>
</div>
<p>Posterior interval</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">LowInterval</span> <span class="o">=</span> <span class="n">beta</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">fem_births</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">births</span><span class="o">-</span><span class="n">fem_births</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">beta</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">fem_births</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">births</span><span class="o">-</span><span class="n">fem_births</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
<span class="n">UppInterval</span> <span class="o">=</span> <span class="n">beta</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">fem_births</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">births</span><span class="o">-</span><span class="n">fem_births</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">beta</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">fem_births</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">births</span><span class="o">-</span><span class="n">fem_births</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>

<span class="nb">round</span><span class="p">(</span><span class="n">LowInterval</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> <span class="nb">round</span><span class="p">(</span><span class="n">UppInterval</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(0.411, 0.474)
</pre></div>
</div>
</div>
</div>
<p>We can also simulate a sample from the posterior to make inferences</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">PosteriorSample</span> <span class="o">=</span> <span class="n">beta</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="n">fem_births</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="n">births</span><span class="o">-</span><span class="n">fem_births</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>

<span class="n">sns</span><span class="o">.</span><span class="n">histplot</span><span class="p">(</span><span class="n">PosteriorSample</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;rebeccapurple&#39;</span><span class="p">,</span> <span class="n">stat</span><span class="o">=</span><span class="s1">&#39;density&#39;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\theta$&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/8fa3d6ad46f197f72ba98e80f44f1140e3d7e88744dfce3afc99be0f1ca7e9f9.png" src="../../_images/8fa3d6ad46f197f72ba98e80f44f1140e3d7e88744dfce3afc99be0f1ca7e9f9.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">quantile</span><span class="p">(</span><span class="n">PosteriorSample</span><span class="p">,</span> <span class="p">[</span><span class="mf">0.025</span><span class="p">,</span> <span class="mf">0.975</span><span class="p">])</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([0.412, 0.473])
</pre></div>
</div>
</div>
</div>
<p>The normal approximation is generally improved by applying it the logit transform, <span class="math notranslate nohighlight">\(\log (\frac{\theta}{1-\theta})\)</span>, which transforms the parameter space from the unit interval to the real line.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">LogitPosteriorSample</span> <span class="o">=</span> <span class="n">logit</span><span class="p">(</span><span class="n">PosteriorSample</span><span class="p">)</span>

<span class="n">sns</span><span class="o">.</span><span class="n">histplot</span><span class="p">(</span><span class="n">LogitPosteriorSample</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;rebeccapurple&#39;</span><span class="p">,</span> <span class="n">stat</span><span class="o">=</span><span class="s1">&#39;density&#39;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;logit$(\theta)$&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/472975dd5184a27aa9b4d83be051d04e50a12fd0a6f2e4c336cd0d5a825e69ef.png" src="../../_images/472975dd5184a27aa9b4d83be051d04e50a12fd0a6f2e4c336cd0d5a825e69ef.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">LowLogitInterval</span> <span class="o">=</span> <span class="n">LogitPosteriorSample</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">LogitPosteriorSample</span><span class="o">.</span><span class="n">std</span><span class="p">()</span>
<span class="n">UppLogitInterval</span> <span class="o">=</span> <span class="n">LogitPosteriorSample</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">LogitPosteriorSample</span><span class="o">.</span><span class="n">std</span><span class="p">()</span>

<span class="n">expit</span><span class="p">(</span><span class="n">LowLogitInterval</span><span class="p">)</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span> <span class="n">expit</span><span class="p">(</span><span class="n">UppLogitInterval</span><span class="p">)</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(0.412, 0.475)
</pre></div>
</div>
</div>
</div>
</section>
<section id="cells-production-of-protein">
<h3><span class="section-number">2.4.3. </span>Cells production of protein<a class="headerlink" href="#cells-production-of-protein" title="Permalink to this heading">#</a></h3>
<p>Assume that a particular population of cells can be in one of three states of protein production: A, B and C, corresponding to low, mid and high production, respectively. If the population is in the state A, we expect that 20% of cells are producing the protein, if it is in the state B we expect 50% and if it is in state C we expect 70%</p>
<p>We take a random sample of 20 cells and verify if each one of them is in production of the protein (the result of the equipment is 1 if the cell is in production and 0 if not). In this sample, we found that 12 cells were in production and the rest were not. What is the probability that the population is in each one of the states?</p>
<p>The next cells show two ways to find the answer to the question.</p>
<div class="cell tag_hide-cell docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell content</span>
<span class="expanded">Hide code cell content</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">theta_A</span> <span class="o">=</span> <span class="mf">0.2</span>
<span class="n">theta_B</span> <span class="o">=</span> <span class="mf">0.5</span>
<span class="n">theta_C</span> <span class="o">=</span> <span class="mf">0.7</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Prob. A: </span><span class="si">{</span><span class="n">theta_A</span><span class="o">**</span><span class="mi">12</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">theta_A</span><span class="p">)</span><span class="o">**</span><span class="mi">8</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="p">(</span><span class="n">theta_A</span><span class="o">**</span><span class="mi">12</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">theta_A</span><span class="p">)</span><span class="o">**</span><span class="mi">8</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">theta_B</span><span class="o">**</span><span class="mi">12</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">theta_B</span><span class="p">)</span><span class="o">**</span><span class="mi">8</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">theta_C</span><span class="o">**</span><span class="mi">12</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">theta_C</span><span class="p">)</span><span class="o">**</span><span class="mi">8</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Prob. B: </span><span class="si">{</span><span class="n">theta_B</span><span class="o">**</span><span class="mi">12</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">theta_B</span><span class="p">)</span><span class="o">**</span><span class="mi">8</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="p">(</span><span class="n">theta_A</span><span class="o">**</span><span class="mi">12</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">theta_A</span><span class="p">)</span><span class="o">**</span><span class="mi">8</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">theta_B</span><span class="o">**</span><span class="mi">12</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">theta_B</span><span class="p">)</span><span class="o">**</span><span class="mi">8</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">theta_C</span><span class="o">**</span><span class="mi">12</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">theta_C</span><span class="p">)</span><span class="o">**</span><span class="mi">8</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Prob. C: </span><span class="si">{</span><span class="n">theta_C</span><span class="o">**</span><span class="mi">12</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">theta_C</span><span class="p">)</span><span class="o">**</span><span class="mi">8</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="p">(</span><span class="n">theta_A</span><span class="o">**</span><span class="mi">12</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">theta_A</span><span class="p">)</span><span class="o">**</span><span class="mi">8</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">theta_B</span><span class="o">**</span><span class="mi">12</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">theta_B</span><span class="p">)</span><span class="o">**</span><span class="mi">8</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">theta_C</span><span class="o">**</span><span class="mi">12</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">theta_C</span><span class="p">)</span><span class="o">**</span><span class="mi">8</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Prob. A: 0.0004
Prob. B: 0.5120
Prob. C: 0.4876
</pre></div>
</div>
</div>
</details>
</div>
<div class="cell tag_hide-cell docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell content</span>
<span class="expanded">Hide code cell content</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">binom</span>

<span class="n">p_A</span> <span class="o">=</span> <span class="n">binom</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="n">theta_A</span><span class="p">)</span>
<span class="n">p_B</span> <span class="o">=</span> <span class="n">binom</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="n">theta_B</span><span class="p">)</span>
<span class="n">p_C</span> <span class="o">=</span> <span class="n">binom</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="n">theta_C</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Prob. A: </span><span class="si">{</span><span class="n">p_A</span><span class="o">/</span><span class="p">(</span><span class="n">p_A</span><span class="o">+</span><span class="n">p_B</span><span class="o">+</span><span class="n">p_C</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Prob. B: </span><span class="si">{</span><span class="n">p_B</span><span class="o">/</span><span class="p">(</span><span class="n">p_A</span><span class="o">+</span><span class="n">p_B</span><span class="o">+</span><span class="n">p_C</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Prob. C: </span><span class="si">{</span><span class="n">p_C</span><span class="o">/</span><span class="p">(</span><span class="n">p_A</span><span class="o">+</span><span class="n">p_B</span><span class="o">+</span><span class="n">p_C</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Prob. A: 0.0004
Prob. B: 0.5120
Prob. C: 0.4876
</pre></div>
</div>
</div>
</details>
</div>
</section>
</section>
<section id="predictive-distributions">
<h2><span class="section-number">2.5. </span>Predictive distributions<a class="headerlink" href="#predictive-distributions" title="Permalink to this heading">#</a></h2>
<p>In many cases we more interested in the behavior of future observations of the phenomenon than on some vector of parameters <span class="math notranslate nohighlight">\(\theta\)</span>. Usually, in frequentist statistics we solve this problem using a punctual estimator of <span class="math notranslate nohighlight">\(\theta\)</span> based on the observed sample, <span class="math notranslate nohighlight">\(\hat{\theta}\)</span>, who is later plug in <span class="math notranslate nohighlight">\(p(Y|\theta)\)</span>, that is, we use <span class="math notranslate nohighlight">\(p(Y|\hat\theta)\)</span> to predict the behavior of future observations.</p>
<p>In Bayesian statistics the problem is solved marginalizing the joint distribution of <span class="math notranslate nohighlight">\(\theta\)</span> and <span class="math notranslate nohighlight">\(Y\)</span></p>
<ul class="simple">
<li><p>Prior predictive distribution:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[p(Y)=\int_\Theta p(Y|\theta)p(\theta)d\theta\]</div>
<p>Once we obtain a sample <span class="math notranslate nohighlight">\(\mathbf{Y}\)</span> it induced a joint distribution for <span class="math notranslate nohighlight">\(Y\)</span> and <span class="math notranslate nohighlight">\(\theta\)</span> conditional on the sample,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
p(Y,\theta|\mathbf{Y}) &amp; = \frac{p(Y,\theta,\mathbf{Y})}{p(\mathbf{Y})} \\
&amp; = p(Y|\theta,\mathbf{Y})\frac{p(\theta,\mathbf{Y})}{p(\mathbf{Y})} \\
&amp; = p(Y|\theta)p(\theta|\mathbf{Y})
\end{aligned}
\end{split}\]</div>
<ul class="simple">
<li><p>Posterior predictive distribution:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
p(Y|\mathbf{Y})=\int_\Theta p(Y|\theta)p(\theta|\mathbf{Y})d\theta
\]</div>
<section id="rule-of-succession">
<h3><span class="section-number">2.5.1. </span>Rule of succession<a class="headerlink" href="#rule-of-succession" title="Permalink to this heading">#</a></h3>
<p>Consider the Beta-Binomial model, and the prior <span class="math notranslate nohighlight">\(\theta\sim\textsf{Beta}(1,1)\)</span>, thus the posterior distribution of <span class="math notranslate nohighlight">\(\theta\)</span> is given by</p>
<div class="math notranslate nohighlight">
\[\theta|Y\sim \textsf{Beta}(Y+1, n-Y+1).\]</div>
<p>Moreover, remember that if <span class="math notranslate nohighlight">\(\theta\)</span> is a random variable with a distribution <span class="math notranslate nohighlight">\(\textsf{Beta}(\alpha,\beta)\)</span>, then</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}(\theta)=\frac{\alpha}{\alpha+\beta},\]</div>
<p>so</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}(\theta|Y)=\frac{Y+1}{n+2}.\]</div>
<p>Assume that we want to know the probability that a new Bernoulli observation <span class="math notranslate nohighlight">\(\tilde{Y}\)</span> takes the value of 1, that is <span class="math notranslate nohighlight">\(\mathbb{P}(\tilde{Y}=1|Y)\)</span>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbb{P}(\tilde{Y}=1|Y) &amp; = \int_0^1 \mathbb{P}(\tilde{Y}=1|\theta)p(\theta|Y)d\theta \\
&amp; = \int_0^1\theta p(\theta|Y)d\theta \\
&amp; = \mathbb{E}(\theta|Y) \\
&amp; = \frac{Y+1}{n+2}.
\end{aligned}
\end{split}\]</div>
<p>If, for example, we have made <span class="math notranslate nohighlight">\(n\)</span> times a Bernoulli experiment without any success (<span class="math notranslate nohighlight">\(Y=0\)</span>), the probability of having success the next time is <span class="math notranslate nohighlight">\(1/(n+2)\)</span>, while the probability of success estimated with classic probability is <span class="math notranslate nohighlight">\(0/n=0\)</span>.</p>
</section>
</section>
<section id="determining-the-hyperparameters-in-the-beta-binomial-model">
<h2><span class="section-number">2.6. </span>Determining the hyperparameters in the Beta-Binomial model<a class="headerlink" href="#determining-the-hyperparameters-in-the-beta-binomial-model" title="Permalink to this heading">#</a></h2>
<p>One of the challenges to solve when we use Bayesian statistics is determine the parameters of the prior distribution, which are called hyperparameters. One way to solve this problem is by interpreting the hyperparameters, and then determine the most appropriate values for them.</p>
<p>For example, consider the model Beta-Binomial, in which the ñikelihood is given by</p>
<div class="math notranslate nohighlight">
\[p(Y|\theta)\propto \theta^a(1-\theta)^b,\]</div>
<p>where <span class="math notranslate nohighlight">\(a\)</span> is the numer of successes and <span class="math notranslate nohighlight">\(b\)</span> is number of fails.</p>
<p>On the other hand, the prior distribution is given by</p>
<div class="math notranslate nohighlight">
\[p(\theta)\propto \theta^{\alpha-1}(1-\theta)^{\beta-1}.\]</div>
<p>Comparing these two expressions, we conclude that <span class="math notranslate nohighlight">\(\alpha-1\)</span> is interpreted as the number of successes <em>a piori</em> and <span class="math notranslate nohighlight">\(\beta-1\)</span> as the number of fails <em>a priori</em>.</p>
<p>Therefore, if we haven’t made any experiment previously, we can set <span class="math notranslate nohighlight">\(\alpha=1\)</span> and <span class="math notranslate nohighlight">\(\beta=1\)</span>. This would mean that <span class="math notranslate nohighlight">\(\theta\sim\textsf{Uniforme}(0,1)\)</span>, which coincides with Laplace’s principle of indifference.</p>
</section>
<section id="normal-convergence-of-the-beta-binomial-model">
<h2><span class="section-number">2.7. </span>Normal convergence of the Beta-Binomial model<a class="headerlink" href="#normal-convergence-of-the-beta-binomial-model" title="Permalink to this heading">#</a></h2>
<p>We know that the Beta-Binomial model satisfies that</p>
<div class="math notranslate nohighlight">
\[\theta|Y\sim\textsf{Beta}(\alpha+Y, \beta+n-Y).\]</div>
<p>Moreover, remember that if <span class="math notranslate nohighlight">\(\theta\)</span> is a random variable with distribution <span class="math notranslate nohighlight">\(\textsf{Beta}(\alpha,\beta)\)</span>, then</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}(\theta)=\frac{\alpha}{\alpha+\beta}\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[\mathbb{V}(\theta)=\frac{\alpha\beta}{(\alpha+\beta)^2(\alpha+\beta+1)}.\]</div>
<p>Thus, we have that</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}(\theta|Y)=\frac{\alpha+Y}{\alpha+\beta+n}\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[\mathbb{V}(\theta|Y)=\frac{(\alpha+Y)(\beta+n-Y)}{(\alpha+\beta+n)^2(\alpha+\beta+n+1)}.\]</div>
<p>Note that when <span class="math notranslate nohighlight">\(n\to\infty\)</span>, then <span class="math notranslate nohighlight">\(Y\to\infty\)</span> and <span class="math notranslate nohighlight">\(n-Y\to\infty\)</span>, always that <span class="math notranslate nohighlight">\(\theta\in(0,1)\)</span>. Therefore when <span class="math notranslate nohighlight">\(n\to\infty\)</span> the value of the hyperparameters are negligible, and</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}(\theta|Y)\approx \frac{Y}{n}\equiv \bar{Y}\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[\mathbb{V}(\theta|Y)\approx \frac{1}{n}\frac{Y}{n}\left(1-\frac{Y}{n}\right)\equiv \frac{1}{n}\bar{Y}(1-\bar{Y}).\]</div>
<p>On the other hand, by central limit theorem and Slutsky’s theorem, we know that</p>
<div class="math notranslate nohighlight">
\[\left(\sqrt{n}\frac{\bar{Y}-\theta}{\sqrt{\bar{Y}(1-\bar{Y})}}\Big|\theta\right)\xrightarrow[n\to\infty]{L}\textsf{Normal}(0,1).\]</div>
<p>Analogously, it is satisfied that</p>
<div class="math notranslate nohighlight">
\[\left(\frac{\theta-\mathbb{E}(\theta|Y)}{\sqrt{\mathbb{V}(\theta|Y)}}\Big|Y\right)\xrightarrow[n\to\infty]{L}\textsf{Normal}(0,1).\]</div>
<p>That is, the posterior distribution converges to a normal random variable.</p>
</section>
<section id="inference-of-a-fair-or-a-biased-coin">
<h2><span class="section-number">2.8. </span>Inference of a fair or a biased coin<a class="headerlink" href="#inference-of-a-fair-or-a-biased-coin" title="Permalink to this heading">#</a></h2>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This example was taken from the notes of Arturo Erdely</p>
</div>
<p>Consider a box with two coins: one fair and one biased. Assume that the biased cooin is built in such way that there is a probability of 3/4 of showing “head”. One person takes one the coins (not necessarily at random) and starts flipping it.</p>
<p>In this case, we can define the measurable space as <span class="math notranslate nohighlight">\(\Omega=\{\text{``The coin shows head''}, \text{``The coin shows tail''}\}\)</span> and take as the sigma-algebra the power set of <span class="math notranslate nohighlight">\(\Omega\)</span>, <span class="math notranslate nohighlight">\(\mathcal{P}(\Omega)\)</span>. In this space, we define the random variable</p>
<div class="math notranslate nohighlight">
\[\begin{split}
Y(\omega)=
\begin{cases}
1 &amp; \text{if }\omega=\text{``The coin shows head''},\\ 
0 &amp; \text{if }\omega=\text{``The coin shows tail''}.
\end{cases}
\end{split}\]</div>
<p>Implicitly, we are not considering all other possible results, like teh case where the coin lands vertically, that we cannot determine the result, etc.</p>
<p>Thus, our parametric space is <span class="math notranslate nohighlight">\(\Theta=\left\lbrace\frac{3}{4}, \frac{1}{2}\right\rbrace\)</span>, our sample space is <span class="math notranslate nohighlight">\(\mathcal{Y}=\{0,1\}\)</span>.</p>
<p>We proceed now to calculate all the distributions presented in the Bayesian framework, beginning with the distributions that can be stablished prior to having access to a sample.</p>
<section id="prior-distribution">
<h3><span class="section-number">2.8.1. </span>Prior distribution<a class="headerlink" href="#prior-distribution" title="Permalink to this heading">#</a></h3>
<p><span class="math notranslate nohighlight">\(\mathbb{P}\left(\theta=\frac{3}{4}\right)=\alpha\)</span>, <span class="math notranslate nohighlight">\(\mathbb{P}\left(\theta=\frac{1}{2}\right)=1-\alpha\)</span>, with <span class="math notranslate nohighlight">\(\alpha\in(0,1)\)</span>. This prior distribution can be written in one line as</p>
<div class="math notranslate nohighlight">
\[p(\theta)=\alpha 1_{\{3/4\}}(\theta)+(1-\alpha)1_{\{1/2\}}(\theta).\]</div>
<p>Note that it depends on the hyperparameter <span class="math notranslate nohighlight">\(\alpha\)</span> which is interpreted as the probability of choosing the biased coin. Note also, that we do not allow it to take the extreme valus of 0 or 1, in such cases we would know without uncertainty what coin was the chosen one, also if <span class="math notranslate nohighlight">\(\alpha=0\)</span> or <span class="math notranslate nohighlight">\(\alpha=1\)</span> we would face problems when we want to calculate some distributions.</p>
</section>
<section id="likelihood">
<h3><span class="section-number">2.8.2. </span>Likelihood<a class="headerlink" href="#likelihood" title="Permalink to this heading">#</a></h3>
<p>We have the random variable <span class="math notranslate nohighlight">\(Y|\theta\sim\textsf{Bernoulli}(\theta)\)</span>, thus the likelihood of our model is</p>
<div class="math notranslate nohighlight">
\[p(Y|\theta)=\theta^Y(1-\theta)^{1-Y}1_{\{0,1\}}(Y).\]</div>
</section>
<section id="prior-predictive-distribution">
<h3><span class="section-number">2.8.3. </span>Prior predictive distribution<a class="headerlink" href="#prior-predictive-distribution" title="Permalink to this heading">#</a></h3>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
p(Y) &amp;=\sum_{\theta\in\Theta}p(Y|\theta)p(\theta) \\
&amp;=\alpha\left(\frac{3}{4}\right)^Y\left(\frac{1}{4}\right)^{1-Y}1_{\{0,1\}}(Y)+(1-\alpha)\left(\frac{1}{2}\right)1_{\{0,1\}}(Y) \\
&amp;= \alpha\left\lbrack\left(\frac{3}{4}-\frac{1}{2}\right)1_{\{1\}}(Y)+\left(\frac{1}{4}-\frac{1}{2}\right)1_{\{0\}}(Y)\right\rbrack+\frac{1}{2}1_{\{0,1\}}(Y) \\
&amp;= \frac{\alpha}{4}\left\lbrack 1_{\{1\}}(Y)-1_{\{0\}}(Y)\right\rbrack+\frac{1}{2}1_{\{0,1\}}(Y).
\end{align*}
\end{split}\]</div>
<p>That is,</p>
<div class="math notranslate nohighlight">
\[p_Y(1)=\frac{1}{2}+\frac{\alpha}{4},\quad p_Y(0)=\frac{1}{2}-\frac{\alpha}{4}.\]</div>
</section>
<section id="likelihood-of-the-sample">
<h3><span class="section-number">2.8.4. </span>Likelihood of the sample<a class="headerlink" href="#likelihood-of-the-sample" title="Permalink to this heading">#</a></h3>
<p>Let be <span class="math notranslate nohighlight">\(Y_1,\ldots,Y_n|\theta\overset{iid}{\sim}\textsf{Bernoulli}(\theta)\)</span>, then</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
p(\mathbf{Y}|\theta) &amp;= \theta^{\sum_{i=1}^n y_i}(1-\theta)^{n-\sum_{i=1}^n y_i}\prod_{i=1}^n 1_{\{0,1\}}(y_i) \\
&amp; = \theta^{\sum_{i=1}^n y_i}(1-\theta)^{n-\sum_{i=1}^n y_i}g(\mathbf{y}).
\end{align*}
\end{split}\]</div>
</section>
<section id="evidence">
<h3><span class="section-number">2.8.5. </span>Evidence<a class="headerlink" href="#evidence" title="Permalink to this heading">#</a></h3>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
p(\mathbf{Y}) &amp;= \sum_{\theta\in\Theta}p(\mathbf{Y}|\theta)p(\theta)\\
&amp;= \alpha\left(\frac{3}{4}\right)^{\sum_{i=1}^n y_i}\left(\frac{1}{4}\right)^{n-\sum_{i=1}^n y_i}g(\mathbf{y})+(1-\alpha)\left(\frac{1}{2}\right)^n g(\mathbf{y})\\\\
&amp;= \left\lbrack\alpha \frac{3^{\sum_{i=1}^n y_i}}{4^n}+(1-\alpha)\frac{1}{2^n}\right\rbrack g(\mathbf{y}).
\end{align*}
\end{split}\]</div>
</section>
<section id="id1">
<h3><span class="section-number">2.8.6. </span>Posterior distribution<a class="headerlink" href="#id1" title="Permalink to this heading">#</a></h3>
<p>Now that we know the likelihood evaluated in the sample, the prior and the evidence, we can calculate the posterior distribution of <span class="math notranslate nohighlight">\(\theta\)</span>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
p(\theta|\mathbf{Y}) &amp;= \frac{p(\mathbf{Y}|\theta)p(\theta)}{p(\mathbf{Y})} \\\\
&amp;= \frac{\theta^{\sum_{i=1}^n y_i}(1-\theta)^{n-\sum_{i=1}^n y_i}\left\lbrack\alpha 1_{\{3/4\}}(\theta)+(1-\alpha)|_{\{1/2\}}(\theta)\right\rbrack}{\alpha \frac{3^{\sum_{i=1}^n y_i}}{4^n}+(1-\alpha)\frac{1}{2^n}}\\\\
&amp;= \frac{[2(1-\theta)]^n\left(\frac{\theta}{1-\theta}\right)^{\sum_{i=1}^n y_i}\left\lbrack\alpha 1_{\{3/4\}}(\theta)+(1-\alpha) |_{\{1/2\}}(\theta)\right\rbrack}{1-\alpha+\alpha\left(\frac{3^{\sum_{i=1}^n y_i}}{2^n}\right)}.
\end{align*}
\end{split}\]</div>
<p>Let be <span class="math notranslate nohighlight">\(v=\frac{\alpha}{1-\alpha}\left(\frac{3^{\sum_{i=1}^n y_i}}{2^n}\right)\)</span>, then</p>
<div class="math notranslate nohighlight">
\[p(\theta|\mathbf{Y})=\frac{[2(1-\theta)]^n\left(\frac{\theta}{1-\theta}\right)^{\sum_{i=1}^n y_i}\left\lbrack\alpha 1_{\{3/4\}}(\theta)+(1-\alpha)1_{\{1/2\}}(\theta)\right\rbrack}{(1-\alpha)(1+v)},\]</div>
<p>that is</p>
<div class="math notranslate nohighlight">
\[p_{\theta|\mathbf{Y}}(1/2|\mathbf{y})=\frac{1}{1+v}\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[p_{\theta|\mathbf{Y}}(3/4|\mathbf{y})=1-\frac{1}{1+v}=\frac{v}{1+v}=\frac{1}{1+v^{-1}}.\]</div>
</section>
<section id="posterior-predictive-distribution">
<h3><span class="section-number">2.8.7. </span>Posterior predictive distribution<a class="headerlink" href="#posterior-predictive-distribution" title="Permalink to this heading">#</a></h3>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
p(Y|\mathbf{Y})&amp;=\sum_{\theta\in\Theta}p(Y|\theta)p(\theta|\mathbf{Y})\\\\
&amp;=\left\lbrack\frac{1}{2}\left(\frac{1}{v+1}\right)+\frac{3^y}{4}\left(\frac{1}{v^{-1}+1}\right)\right\rbrack 1_{\{0,1\}}(y)\\\\
&amp;=\left\lbrack\frac{2}{4(v+1)}+\frac{3^yv}{4(v+1)}\right\rbrack 1_{\{0,1\}}(y)\\\\
&amp;=\frac{3^yv+2}{4(v+1)} 1_{\{0,1\}}(y),
\end{align*}
\end{split}\]</div>
<p>that is</p>
<div class="math notranslate nohighlight">
\[p_{Y|\mathbf{Y}}(1|\mathbf{y})=\frac{3v+2}{4(v+1)},\quad p_{Y|\mathbf{Y}}(0|\mathbf{y})=\frac{v+2}{4(v+1)}\]</div>
<div class="tip admonition">
<p class="admonition-title">Likelihood and likelihood of the sample</p>
<p>In this book I make distinction between the likelihood and the likelihood of the sample. However, this is not the rule but the exception, most of the Bayesian literature called both functions simply as likelihood. I have seen that students that face for the first time Bayesian statistics can be confused about which likelihood they should use when this distinction is not done. I don’t complain them, the first time I was confused as well.</p>
</div>
</section>
</section>
<section id="there-is-no-free-lunch">
<h2><span class="section-number">2.9. </span>There is no free lunch<a class="headerlink" href="#there-is-no-free-lunch" title="Permalink to this heading">#</a></h2>
<p>When we make inference using frequentist statistic the procedures are usually justified by an asymptotic analysis of the method. As a consequence, it’s performance for small samples is questionable. Meanwhile, the Bayesian statistic is valid for any sample size. This doesn’t mean that having more data is useless, but the opposite. The price to pay for this power is the dependence on the prior information. A non-reliable prior distribution compromises the results.</p>
<div class="tip admonition">
<p class="admonition-title">Historical discussion about the prior</p>
<p>Historically, some detractors of the Bayesian statistics have argumented about the arbitrariness of the prior distribution. It is true that prior distributions can be flexible enough to code different types of information. Then, if the prior can be anything, isn’t it possible to obtain any answer that you want? The answer is yes.</p>
<p>However, the likelihood is also a subjective model that is impose by the agent, and eventually with a large sample the effect of the prior distribution would be eventually buried. Thus, if your objective is to modify the results is easier modifying the likelihood.</p>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./content/part1"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="Chapter1.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">1. </span>Bayesian philosophy</p>
      </div>
    </a>
    <a class="right-next"
       href="../../References.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">References</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#notation">2.1. Notation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#posterior-distribution">2.2. Posterior distribution</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#beta-binomial">2.3. Beta-Binomial</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example">2.3.1. Example</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#principle-of-indifference">2.4. Principle of indifference</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#proportion-of-girls-births">2.4.1. Proportion of girls’ births</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#probability-of-a-girl-birth-given-placenta-previa">2.4.2. Probability of a girl birth given placenta previa</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cells-production-of-protein">2.4.3. Cells production of protein</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#predictive-distributions">2.5. Predictive distributions</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#rule-of-succession">2.5.1. Rule of succession</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#determining-the-hyperparameters-in-the-beta-binomial-model">2.6. Determining the hyperparameters in the Beta-Binomial model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#normal-convergence-of-the-beta-binomial-model">2.7. Normal convergence of the Beta-Binomial model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#inference-of-a-fair-or-a-biased-coin">2.8. Inference of a fair or a biased coin</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#prior-distribution">2.8.1. Prior distribution</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#likelihood">2.8.2. Likelihood</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#prior-predictive-distribution">2.8.3. Prior predictive distribution</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#likelihood-of-the-sample">2.8.4. Likelihood of the sample</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#evidence">2.8.5. Evidence</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">2.8.6. Posterior distribution</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#posterior-predictive-distribution">2.8.7. Posterior predictive distribution</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#there-is-no-free-lunch">2.9. There is no free lunch</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Irving Gómez-Méndez
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>