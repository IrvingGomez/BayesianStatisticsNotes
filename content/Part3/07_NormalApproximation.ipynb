{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normal approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume that $Y_1,\\ldots,Y_n$ are i.i.d. r.v. with common density $f(Y)$, but we, in our ignorance, model them with the likelihood $p(Y|\\theta)$. Let be $\\theta_0$ the value that minimizes the Kullback-Leibler divergence between $f(Y)$ and $p(Y|\\theta)$.\n",
    "\n",
    "Under regularity conditions, for $n$ sufficiently large, it can be shown that\n",
    "\n",
    "$$\\theta|\\mathbf{Y}\\overset{\\cdot}{\\sim}\\textsf{Normal}\\left(\\theta_0, (nJ(\\theta_0))^{-1}\\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{seealso}\n",
    "Entropy and the Kullback-Leibler divergence are fundamental concepts in the fields of machine learning, statistics and information theory. For interested readers in entropy and its use in machine learning algorithms, I recommend the following video.\n",
    "\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/YtebGVx-Fxw?si=T8m9VQFd6esm2P6d\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>\n",
    "\n",
    "For those interested in the relationship between the entropy,the Kullback-Leibler divergence and their relationship with maximum likelihood, you can find some notes that I wrote [here should be a link to my notes, meanwhile there is a link to wikipedia](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence).\n",
    "\n",
    "Since entropy and the Kullback-Leibler divergence are concepts strongly associated with the field of information theory, I recommend the following reference for those interested in the field. **I have to add the references.**\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observed Fisher information\n",
    "\n",
    "If $\\hat{\\theta}$ denotes the mode of the posterior distribution, known as the *maximum a posteriori* (MAP), under the same regularity conditions and for $n$ sufficently large,\n",
    "\n",
    "$$\\hat{\\theta}\\approx\\theta_0.$$\n",
    "\n",
    "On the other hand, we define the observed Fisher information, in the Bayesin framework and under regularity conditions, as\n",
    "\n",
    "$$I(\\hat{\\theta})=-\\left[\\frac{d^2}{d\\theta^2}\\log p(\\theta|\\mathbf{Y})\\right]_{\\theta=\\hat{\\theta}}.$$\n",
    "\n",
    "Note that we can write this expression as:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "I(\\hat{\\theta}) &= -\\left[\\frac{d^2}{d\\theta^2}\\log p(\\theta|\\mathbf{Y})\\right]_{\\theta=\\hat{\\theta}} \\\\\\\\\n",
    "& = -\\frac{d^2}{d\\theta^2}\\Big[\\log p(\\mathbf{Y}|\\theta) + \\log p(\\theta) - \\log p(\\mathbf{Y})\\Big]_{\\theta=\\hat{\\theta}} \\\\\\\\\n",
    "& = -\\left[\\frac{d^2}{d\\theta^2}\\log p(\\mathbf{Y}|\\theta)\\right]_{\\theta=\\hat{\\theta}} - \\left[\\frac{d^2}{d\\theta^2} \\log p(\\theta)\\right]_{\\theta=\\hat{\\theta}},\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "because we assume that $Y_1,\\ldots Y_n$ are i.i.d., then\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "I(\\hat{\\theta}) &= -\\sum_{i=1}^n\\left[\\frac{d^2}{d\\theta^2}\\log p(Y_i|\\theta)\\right]_{\\theta=\\hat{\\theta}} - \\left[\\frac{d^2}{d\\theta^2} \\log p(\\theta)\\right]_{\\theta=\\hat{\\theta}} \\\\\\\\\n",
    "&= -n\\left[\\frac{1}{n}\\sum_{i=1}^n\\frac{d^2}{d\\theta^2}\\log p(Y_i|\\theta)\\right]_{\\theta=\\hat{\\theta}} - \\left[\\frac{d^2}{d\\theta^2} \\log p(\\theta)\\right]_{\\theta=\\hat{\\theta}}.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "By law of large numbers, under regularity conditions and for $n$ sufficiently large,\n",
    "\n",
    "$$-\\frac{1}{n}\\sum_{i=1}^n\\left[\\frac{d^2}{d\\theta^2}\\log p(Y_i|\\theta)\\right]_{\\theta=\\hat{\\theta}} \\approx J(\\theta_0),$$\n",
    "\n",
    "also the second term on the right side becomes negligible.\n",
    "\n",
    "Thus, under regularity conditions and for $n$ sufficiently large,\n",
    "\n",
    "$$I(\\hat{\\theta})\\approx nJ(\\theta_0)$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\\theta|\\mathbf{Y}\\overset{\\cdot}{\\sim}\\textsf{Normal}\\left(\\hat{\\theta},\\left(I(\\hat{\\theta})\\right)^{-1}\\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Region of approximately $(1-\\alpha)$ posterior probability\n",
    "\n",
    "On the other hand, consider the Taylor series of the log-posterior around the MAP $\\hat{\\theta}$,\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\log p(\\theta|\\mathbf{Y})-\\log p(\\hat{\\theta}|\\mathbf{Y}) &\\approx \\frac{1}{2}(\\theta-\\hat{\\theta})^T\\left[\\frac{d^2}{d\\theta^2}\\log p(\\theta|\\mathbf{Y})\\right]_{\\theta=\\hat{\\theta}}(\\theta-\\hat{\\theta})\\\\\\\\\n",
    "\\Rightarrow -2\\log \\frac{p(\\theta|\\mathbf{Y})}{p(\\hat{\\theta}|\\mathbf{Y})} &\\approx (\\theta-\\hat{\\theta})^T\\left[-\\frac{d^2}{d\\theta^2}\\log p(\\theta|\\mathbf{Y})\\right]_{\\theta=\\hat{\\theta}}(\\theta-\\hat{\\theta}).\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Thus, if $\\theta$ has dimension $k$,\n",
    "\n",
    "$$-2\\log \\frac{p(\\theta|\\mathbf{Y})}{p(\\hat{\\theta}|\\mathbf{Y})}\\overset{\\cdot}{\\sim}\\chi^2_k.$$\n",
    "\n",
    "Let be $q_{\\chi^2_k}^{1-\\alpha}$ the quantile of probability $1-\\alpha$ of a $\\chi^2_k$ distribution, i.e.\n",
    "\n",
    "$$\\mathbb{P}\\left(\\chi^2_k\\leq q_{\\chi^2_k}^{1-\\alpha}\\right)=1-\\alpha.$$\n",
    "\n",
    "Therefore,\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbb{P}\\left[-2\\log \\frac{p(\\theta|\\mathbf{Y})}{p(\\hat{\\theta}|\\mathbf{Y})}\\leq q_{\\chi^2_k}^{1-\\alpha}\\right] & \\approx 1-\\alpha \\\\\\\\\n",
    "\\Rightarrow \\mathbb{P}\\left[\\frac{p(\\theta|\\mathbf{Y})}{p(\\hat{\\theta}|\\mathbf{Y})}\\geq \\exp\\left\\{-\\frac{q_{\\chi^2_k}^{1-\\alpha}}{2}\\right\\}\\right] & \\approx 1-\\alpha.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "That is, the region of $\\Theta$, given by\n",
    "\n",
    "$$R(\\Theta)=\\left\\{\\theta: \\frac{p(\\theta|\\mathbf{Y})}{p(\\hat{\\theta}|\\mathbf{Y})}\\geq \\exp\\left\\{-\\frac{q_{\\chi^2_k}^{1-\\alpha}}{2}\\right\\}\\right\\}$$\n",
    "\n",
    "corresponds to a region of approximately $1-\\alpha$ posterior probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal approximation of the Beta-Binomial model\n",
    "\n",
    "Let be $Y_1,\\ldots,Y_n|\\theta\\overset{iid}{\\sim}\\textsf{Bernoulli}(\\theta)$, and consider the prior $\\textsf{Beta}(\\alpha,\\beta)$, then $\\theta|\\mathbf{Y}\\sim\\textsf{Beta}(\\alpha_n,\\beta_n)$, where $\\alpha_n=\\alpha+\\sum_{i=1}^n Y_i$ and $\\beta_n=\\beta+n-\\sum_{i=1}^n Y_i$, so\n",
    "\n",
    "$$p(\\theta|\\mathbf{Y})=\\text{constant}\\times\\theta^{\\alpha_n-1}(1-\\theta)^{\\beta_n-1}$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\\log p(\\theta|\\mathbf{Y})=\\text{constant}+(\\alpha_n-1)\\log\\theta+(\\beta_n-1)\\log(1-\\theta).$$\n",
    "\n",
    "We now compute the first derivative,\n",
    "\n",
    "$$\\frac{d}{d\\theta}\\log p(\\theta|\\mathbf{Y})=\\frac{\\alpha_n-1}{\\theta}-\\frac{\\beta_n-1}{1-\\theta}.$$\n",
    "\n",
    "Assume that $\\alpha_n,\\beta_n>1$. Then, the MAP of $\\theta$, $\\hat{\\theta}$, satisfies\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{1-\\hat{\\theta}}{\\hat{\\theta}} & = \\frac{\\beta_n-1}{\\alpha_n-1} \\\\\\\\\n",
    "\\Rightarrow \\frac{1}{\\hat{\\theta}} & = \\frac{\\beta_n-1}{\\alpha_n-1}+1 \\\\\\\\\n",
    "\\Rightarrow \\hat{\\theta} & = \\frac{\\alpha_n-1}{\\alpha_n+\\beta_n-2},\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "which, of course, corresponds to the mode of a distribution $\\textsf{Beta}(\\alpha_n,\\beta_n)$.\n",
    "\n",
    "Now, we calculate the second derivative and evaluate it in the MAP,\n",
    "\n",
    "$$\\left[\\frac{d^2}{d\\theta^2}\\log p(\\theta|\\mathbf{Y})\\right]_{\\hat{\\theta}}=-\\frac{\\alpha_n-1}{\\hat{\\theta}^2}-\\frac{\\beta_n-1}{(1-\\hat{\\theta})^2},$$\n",
    "\n",
    "note that\n",
    "\n",
    "$$1-\\hat{\\theta}=\\hat{\\theta}\\left(\\frac{\\beta_n-1}{\\alpha_n-1}\\right),$$\n",
    "\n",
    "then\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\left[\\frac{d^2}{d\\theta^2}\\log p(\\theta|\\mathbf{Y})\\right]_{\\hat{\\theta}} & = -\\frac{\\alpha_n-1}{\\hat{\\theta}^2}-\\frac{(\\alpha_n-1)^2}{\\hat{\\theta}^2(\\beta_n-1)} \\\\\\\\\n",
    "& = -\\frac{\\alpha_n-1}{\\hat{\\theta}^2}\\left(1+\\frac{\\alpha_n-1}{\\beta_n-1}\\right) \\\\\\\\\n",
    "& = -\\frac{\\alpha_n-1}{\\hat{\\theta}^2}\\left(\\frac{\\alpha_n+\\beta_n-2}{\\beta_n-1}\\right) \\\\\\\\\n",
    "& = -\\frac{\\alpha_n+\\beta_n-2}{\\hat{\\theta}(1-\\hat{\\theta})},\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "the last equality is obatined noting that\n",
    "\n",
    "$$\\frac{\\alpha_n-1}{\\beta_n-1}=\\frac{\\hat{\\theta}}{1-\\hat{\\theta}}.$$\n",
    "\n",
    "Thus,\n",
    "\n",
    "$$\\theta|\\mathbf{Y}\\overset{\\cdot}{\\sim}\\textsf{Normal}\\left(\\hat{\\theta},\\frac{\\hat{\\theta}(1-\\hat{\\theta})}{\\alpha_n+\\beta_n-2}\\right),$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\\hat{\\theta}=\\frac{\\alpha_n-1}{\\alpha_n+\\beta_n-2}.$$\n",
    "\n",
    "If we set $\\alpha=1$ and $\\beta=1$, then $\\alpha_n=1+\\sum_{i=1}^n Y_i$, $\\beta_n=1+n-\\sum_{i=1}^n Y_i$. So, $\\hat{\\theta}=\\bar{Y}$ and\n",
    "\n",
    "$$\\theta|\\mathbf{Y}\\overset{\\cdot}{\\sim}\\textsf{Normal}\\left(\\hat{\\theta},\\frac{\\hat{\\theta}(1-\\hat{\\theta})}{n}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code [07_NormalApproximation.ipynb](https://github.com/IrvingGomez/BayesianStatistics/blob/main/Codes/07_NormalApproximation.ipynb) within the [repository of the course](https://github.com/IrvingGomez/BayesianStatistics), there are two examples of the normal approximation for the posterior distribution, on for the Beta-Binomial model and the other for the Gamma-Exponential model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal approximation for the normal likelihood considering the conjugate prior\n",
    "\n",
    "As mentioned in [Chapter 4](../Part2/04_MultiparametricModels), if $Y_1,\\ldots,Y_n|\\mu,\\sigma^2\\overset{iid}{\\sim}\\textsf{Normal}(\\mu,\\sigma^2)$, then the conjugate prior is given by the NI$\\chi^2$ distribution:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mu|\\sigma^2 &\\sim \\textsf{Normal}(\\mu_0,\\sigma^2/\\kappa_0) \\\\\n",
    "\\sigma^2 &\\sim \\textsf{Inversa}-\\chi^2(\\nu_0,\\sigma_0^2),\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "whose posterior distributions are given by\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mu|\\sigma^2,\\mathbf{Y} &\\sim \\textsf{Normal}(\\mu_n,\\sigma^2/\\kappa_n)\\\\\n",
    "\\sigma^2|\\mathbf{Y} &\\sim \\textsf{Inversa}-\\chi^2(\\nu_n,\\sigma_n^2),\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mu_n &= \\frac{\\kappa_0}{\\kappa_0+n}\\mu_0 + \\frac{n}{\\kappa_0+n}\\bar{y} \\\\\n",
    "\\kappa_n &= \\kappa_0+n \\\\\n",
    "\\nu_n &= \\nu_0+n \\\\\n",
    "\\nu_n\\sigma_n^2 &= \\nu_0\\sigma_0^2+(n-1)s^2+\\frac{\\kappa_0 n}{\\kappa_0+n}(\\bar{y}-\\mu_0)^2.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Then\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "p(\\mu,\\sigma^2|\\mathbf{Y})= & \\text{constant} \\times(\\sigma^2)^{-1/2}\\exp\\left\\{-\\frac{\\kappa_n}{2\\sigma^2}(\\mu-\\mu_n)^2\\right\\} \\\\\\\\\n",
    "& \\times(\\sigma^2)^{-(\\nu_n/2+1)}\\exp\\left\\{-\\frac{\\nu_n\\sigma_n^2}{2\\sigma^2}\\right\\}1_{\\mathbb{R}}(\\mu)1_{(0,\\infty)}(\\sigma^2),\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "so\n",
    "\n",
    "$$\\log p(\\mu,\\sigma^2|\\mathbf{Y})=\\text{constant}-\\left(\\frac{\\nu_n+3}{2}\\right)\\log(\\sigma^2)-\\frac{\\kappa_n}{2\\sigma^2}(\\mu-\\mu_n)^2-\\frac{\\nu_n\\sigma_n^2}{2\\sigma^2}.$$\n",
    "\n",
    "We now calculate the first derivative of the log-posterior with respect to $\\mu$\n",
    "\n",
    "$$\\frac{\\partial}{\\partial\\mu}\\log p(\\mu,\\sigma^2|\\mathbf{Y})=-\\frac{\\kappa_n}{\\sigma^2}(\\mu-\\mu_n),$$\n",
    "\n",
    "where we can recognize easily that the MAP of $\\mu$ is given by $\\hat{\\mu}=\\mu_n$.\n",
    "\n",
    "The first derivative of the log-posterior with respect to $\\sigma^2$ is given by\n",
    "\n",
    "$$\\frac{\\partial}{\\partial\\sigma^2}\\log p(\\mu,\\sigma^2|\\mathbf{Y})=-\\left(\\frac{\\nu_n+3}{2}\\right)\\frac{1}{\\sigma^2}+\\frac{\\kappa_n}{2\\sigma^4}(\\mu-\\mu_n)^2+\\frac{\\nu_n\\sigma_n^2}{2\\sigma^4}.$$\n",
    "\n",
    "Then, the MAP of $\\sigma^2$, $\\hat{\\sigma}^2$, satisfies that\n",
    "\n",
    "$$-\\left(\\frac{\\nu_n+3}{2}\\right)\\frac{1}{\\hat\\sigma^2}+\\frac{\\nu_n\\sigma_n^2}{2\\hat\\sigma^4}=0,$$\n",
    "\n",
    "where we get\n",
    "\n",
    "$$\\hat{\\sigma}^2=\\frac{\\nu_n}{\\nu_n+3}\\sigma_n^2.$$\n",
    "\n",
    "We now calculate the second derivatives of the log-posterior:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial^2}{\\partial\\mu^2}\\log p(\\mu,\\sigma^2|\\mathbf{Y}) & = -\\frac{\\kappa_n}{\\sigma^2}, \\\\\\\\\n",
    "\\frac{\\partial^2}{\\partial\\mu\\partial\\sigma}\\log p(\\mu,\\sigma^2|\\mathbf{Y}) & = \\frac{\\kappa_n}{\\sigma^4}(\\mu-\\mu_n), \\\\\\\\\n",
    "\\frac{\\partial^2}{\\partial(\\sigma^2)^2}\\log p(\\mu,\\sigma^2|\\mathbf{Y}) & = \\frac{\\nu_n+3}{2\\sigma^4}-\\frac{\\kappa_n}{\\sigma^6}(\\mu-\\mu_n)^2-\\frac{\\nu_n\\sigma_n^2}{\\sigma^6}.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "And evaluate these expressions in the MAP of the parameters, $(\\hat{\\mu},\\hat{\\sigma}^2)$,\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\left[\\frac{\\partial^2}{\\partial\\mu^2}\\log p(\\mu,\\sigma^2|\\mathbf{Y})\\right]_{(\\hat{\\mu},\\hat{\\sigma}^2)} & = -\\frac{\\kappa_n}{\\hat\\sigma^2}, \\\\\\\\\n",
    "\\left[\\frac{\\partial^2}{\\partial\\mu\\partial\\sigma^2}\\log p(\\mu,\\sigma^2|\\mathbf{Y})\\right]_{(\\hat{\\mu},\\hat{\\sigma}^2)} & = 0, \\\\\\\\\n",
    "\\left[\\frac{\\partial^2}{\\partial(\\sigma^2)^2}\\log p(\\mu,\\sigma^2|\\mathbf{Y})\\right]_{(\\hat{\\mu},\\hat{\\sigma}^2)} & = \\frac{\\nu_n+3}{2\\hat\\sigma^4}-\\frac{\\nu_n\\sigma_n^2}{\\hat\\sigma^6} \\\\\n",
    "& = \\frac{\\nu_n+3}{2\\hat\\sigma^4}-\\frac{\\cancel{\\nu_n\\sigma_n^2}}{\\hat\\sigma^4\\cancel{\\nu_n\\sigma_n^2}}(\\nu_n+3) \\\\\n",
    "& = -\\frac{\\nu_n+3}{2\\hat{\\sigma}^4}.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Then,\n",
    "\n",
    "$$\\mu,\\sigma^2|\\mathbf{Y}\\overset{\\cdot}{\\sim}\\textsf{Normal}\\left(\n",
    "\\begin{pmatrix}\n",
    "    \\hat{\\mu} \\\\ \\hat{\\sigma}^2\n",
    "\\end{pmatrix},\n",
    "\\begin{pmatrix}\n",
    "    \\frac{\\hat{\\sigma}^2}{\\kappa_n} & 0 \\\\ 0 & \\frac{2}{\\nu_n+3}\\hat{\\sigma}^4\n",
    "\\end{pmatrix}\n",
    "\\right),$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\\hat{\\mu}=\\mu_n\\text{ and }\\hat{\\sigma}^2=\\frac{\\nu_n}{\\nu_n+3}\\sigma_n^2.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal aprroximation considering the reference prior\n",
    "\n",
    "As discussed in [Chapter 6](../Part3/06_ReferenceAnalysis), the reference prior is given by\n",
    "\n",
    "$$p(\\mu,\\sigma^2)\\propto\\frac{1}{\\sigma^2}1_{\\mathbb{R}}(\\mu)1_{(0,\\infty)}(\\sigma^2),$$\n",
    "\n",
    "which is obateined setting the hyperparameters in the following values\n",
    "\n",
    "$$\\kappa_0=0,\\quad \\mu_0\\in\\mathbb{R},\\quad\\nu_0=-1,\\quad\\sigma_0^2=0,$$\n",
    "\n",
    "then\n",
    "\n",
    "$$\\kappa_n=n,\\quad \\mu_n=\\bar{y},\\quad\\nu_n=n-1,\\quad\\sigma_n^2=s^2,$$\n",
    "\n",
    "with\n",
    "\n",
    "$s^2=\\frac{1}{n-1}\\sum_{i=1}^n(y_i-\\bar{y})^2$.\n",
    "\n",
    "Therefore, if we take the reference prior, we get that\n",
    "\n",
    "$$\\mu,\\sigma^2|\\mathbf{Y}\\overset{\\cdot}{\\sim}\\textsf{Normal}\\left(\n",
    "\\begin{pmatrix}\n",
    "    \\bar{y} \\\\ \\hat{\\sigma}^2\n",
    "\\end{pmatrix},\n",
    "\\begin{pmatrix}\n",
    "    \\frac{\\hat{\\sigma}^2}{n} & 0 \\\\ 0 & \\frac{2}{n+2}\\hat{\\sigma}^4\n",
    "\\end{pmatrix}\n",
    "\\right),\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\\hat{\\sigma}^2=\\frac{n-1}{n+2}s^2.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
